#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤ —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
"""

import os
import sys
import json
import requests
import whisper
import cv2
import numpy as np
import base64
from dotenv import load_dotenv
from openai import OpenAI
from PIL import Image
import re

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
from adaptive_screenshot_extractor import AdaptiveScreenshotExtractor
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞  
from smart_transcript_extractor import SmartTranscriptExtractor
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
from cache_manager import CacheManager

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
try:
    from chronological_transcript_processor import ChronologicalTranscriptProcessor
    CHRONOLOGICAL_AVAILABLE = True
except ImportError:
    CHRONOLOGICAL_AVAILABLE = False
    print("‚ö†Ô∏è  –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–µ—Ä–µ–∏–º–µ–Ω—É–π—Ç–µ chronological-transcript-processor.py –≤ chronological_transcript_processor.py")

def extract_audio_from_video(video_path, output_dir="temp", cache_manager=None):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à, –µ—Å–ª–∏ –º–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
    if cache_manager:
        cached_audio = cache_manager.get_cached_audio(video_path)
        if cached_audio:
            return cached_audio
    
    try:
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        audio_path = os.path.join(output_dir, f"{video_name}_audio.wav")
        
        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –∏–∑ {video_path}...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º cv2 –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±)
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ—Ñ–∞–π–ª: {video_path}")
        
        # –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º ffmpeg —á–µ—Ä–µ–∑ os.system
        import subprocess
        
        # –ö–æ–º–∞–Ω–¥–∞ ffmpeg –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ
        cmd = [
            "ffmpeg", "-y",  # -y –¥–ª—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞
            "-i", video_path,
            "-vn",  # –Ω–µ –≤–∫–ª—é—á–∞—Ç—å –≤–∏–¥–µ–æ
            "-acodec", "pcm_s16le",  # –∫–æ–¥–µ–∫ –¥–ª—è wav
            "-ar", "16000",  # —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ 16kHz (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Whisper)
            "-ac", "1",  # –º–æ–Ω–æ
            audio_path
        ]
        
        print("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ...")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"–û—à–∏–±–∫–∞ ffmpeg: {result.stderr}")
            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ moviepy
            try:
                from moviepy.editor import VideoFileClip
                print("–ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ moviepy...")
                video_clip = VideoFileClip(video_path)
                audio_clip = video_clip.audio
                audio_clip.write_audiofile(audio_path, verbose=False, logger=None)
                audio_clip.close()
                video_clip.close()
            except ImportError:
                print("–î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å ffmpeg –∏–ª–∏ moviepy")
                print("–£—Å—Ç–∞–Ω–æ–≤–∫–∞: pip install moviepy")
                return None
        
        if os.path.exists(audio_path):
            print(f"–ê—É–¥–∏–æ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {audio_path}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à, –µ—Å–ª–∏ –º–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
            if cache_manager:
                cached_path = cache_manager.save_audio_cache(video_path, audio_path)
                return cached_path
            
            return audio_path
        else:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∞—É–¥–∏–æ")
            return None
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞—É–¥–∏–æ: {e}")
        return None

def create_transcript_with_whisper(audio_file, model_size="base", video_path=None, cache_manager=None):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ —Å –ø–æ–º–æ—â—å—é Whisper"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à, –µ—Å–ª–∏ –º–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
    if cache_manager and video_path:
        cached_transcript = cache_manager.get_cached_transcript(video_path)
        if cached_transcript:
            return cached_transcript
    
    try:
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper ({model_size})...")
        model = whisper.load_model(model_size)
        
        print("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ...")
        result = model.transcribe(audio_file, language="ru")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        segments = []
        for segment in result["segments"]:
            segments.append({
                "text": segment["text"],
                "start": segment["start"],
                "duration": segment["end"] - segment["start"]
            })
        
        full_text = result["text"]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à, –µ—Å–ª–∏ –º–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
        if cache_manager and video_path:
            cache_manager.save_transcript_cache(video_path, segments, full_text)
        
        return segments, full_text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ —Å Whisper: {e}")
        return None, None

def extract_screenshots_traditional(video_path, output_dir, interval=30, api_key=None):
    """–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–¥–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ —Ñ–∞–π–ª
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ {video_path}")
        return []
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∏–¥–µ–æ
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    frame_interval = int(fps * interval)  # –ö–∞–¥—Ä—ã —á–µ—Ä–µ–∑ –∫–∞–∂–¥—ã–µ interval —Å–µ–∫—É–Ω–¥
    
    print(f"–ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ: –≤—Å–µ–≥–æ {total_frames} –∫–∞–¥—Ä–æ–≤, FPS: {fps}, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.1f} —Å–µ–∫")
    print(f"–ë—É–¥–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–æ ~{int(duration // interval)} —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤")
    
    saved_screenshots = []
    frame_count = 0
    screenshot_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–¥—Ä –∫–∞–∂–¥—ã–µ interval —Å–µ–∫—É–Ω–¥
        if frame_count % frame_interval == 0:
            current_time = frame_count / fps
            timestamp = int(current_time)
            
            # –ò–º—è —Ñ–∞–π–ª–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
            screenshot_path = f"{output_dir}/screenshot_{timestamp:05d}s.jpg"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–¥—Ä —Å –≤—ã—Å–æ–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º
            cv2.imwrite(screenshot_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
            
            description = None
            if api_key:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç —Å –ø–æ–º–æ—â—å—é GPT-4V
                description = analyze_screenshot_with_gpt4v(screenshot_path, api_key)
            
            saved_screenshots.append((screenshot_path, timestamp, description, "periodic"))
            screenshot_count += 1
            
            minutes = timestamp // 60
            seconds = timestamp % 60
            print(f"–°–∫—Ä–∏–Ω—à–æ—Ç {screenshot_count}: {minutes}:{seconds:02d} - {screenshot_path}")
        
        frame_count += 1
    
    cap.release()
    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(saved_screenshots)} —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤")
    return saved_screenshots

def extract_screen_content(image_path, api_key):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –∫–æ–¥, —Ç–∞–±–ª–∏—Ü—ã —Å —ç–∫—Ä–∞–Ω–∞"""
    try:
        # –ö–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç OpenAI
        client = OpenAI(api_key=api_key)
        
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        prompt = """–ò–∑–≤–ª–µ–∫–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å —ç—Ç–æ–≥–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞:

1. **–ö–æ–¥** - –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –∫–æ–¥, —Å–∫–æ–ø–∏—Ä—É–π –µ–≥–æ —Ç–æ—á–Ω–æ
2. **–¢–µ–∫—Å—Ç** - –≤–µ—Å—å —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —ç–∫—Ä–∞–Ω–µ
3. **–¢–∞–±–ª–∏—Ü—ã** - –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
4. **–î–∏–∞–≥—Ä–∞–º–º—ã** - –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ö–µ–º –∏ –¥–∏–∞–≥—Ä–∞–º–º
5. **UI —ç–ª–µ–º–µ–Ω—Ç—ã** - –Ω–∞–∑–≤–∞–Ω–∏—è –∫–Ω–æ–ø–æ–∫, –º–µ–Ω—é, –ø–æ–ª–µ–π

–í–µ—Ä–Ω–∏ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{
    "content_type": "code/text/table/diagram/ui",
    "extracted_text": "–≤–µ—Å—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
    "code_snippets": ["—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–¥–∞ –µ—Å–ª–∏ –µ—Å—Ç—å"],
    "table_data": "–¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü –µ—Å–ª–∏ –µ—Å—Ç—å",
    "ui_elements": ["—ç–ª–µ–º–µ–Ω—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"],
    "description": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"
}

–í–ê–ñ–ù–û: –ò–∑–≤–ª–µ–∫–∞–π —Ç–µ–∫—Å—Ç –¢–û–ß–ù–û –∫–∞–∫ –Ω–∞–ø–∏—Å–∞–Ω–æ, –≤–∫–ª—é—á–∞—è –∫–æ–¥ –∏ –¥–∞–Ω–Ω—ã–µ."""
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ GPT-4V
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}",
                        },
                    },
                ],
            }],
            response_format={"type": "json_object"},
            max_tokens=1500
        )
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —ç–∫—Ä–∞–Ω–∞: {e}")
        return None

def analyze_screenshot_with_gpt4v(image_path, api_key):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç —Å –ø–æ–º–æ—â—å—é GPT-4V"""
    try:
        # –ö–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç OpenAI
        client = OpenAI(api_key=api_key)
        
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
        prompt = """–û–ø–∏—à–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —ç—Ç–æ–≥–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞ –∏–∑ –≤–∏–¥–µ–æ –≤—Å—Ç—Ä–µ—á–∏. –í–∫–ª—é—á–∏:
1. –ß—Ç–æ –≤–∏–¥–Ω–æ –Ω–∞ —ç–∫—Ä–∞–Ω–µ (–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å, –¥–æ–∫—É–º–µ–Ω—Ç—ã, –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è, –∫–æ–¥ –∏ —Ç.–¥.)
2. –û—Å–Ω–æ–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
3. –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç, –∫—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ–¥–∞–π –µ–≥–æ —Å—É—Ç—å
4. –û–±—â–∞—è —Ç–µ–º–∞/–∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–µ–≥–æ

–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º, –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."""
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ GPT-4V
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}",
                        },
                    },
                ],
            }],
            max_tokens=500
        )
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        description = response.choices[0].message.content.strip()
        return description
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞ —Å GPT-4V: {e}")
        return None

def analyze_content_with_gpt(transcript_text, api_key, video_title="–õ–æ–∫–∞–ª—å–Ω–æ–µ –≤–∏–¥–µ–æ", screenshots_info=None, video_path=None, cache_manager=None):
    """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é GPT"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à, –µ—Å–ª–∏ –º–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
    if cache_manager and video_path:
        cached_analysis = cache_manager.get_cached_analysis(video_path, "basic")
        if cached_analysis:
            return cached_analysis
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞—Ö –≤ –ø—Ä–æ–º–ø—Ç
    screenshots_summary = ""
    if screenshots_info and len(screenshots_info) > 0:
        screenshots_summary = "\n\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–∞—Ö –≤–∏–¥–µ–æ (–Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤):\n"
        for i, (_, timestamp, description, reason) in enumerate(screenshots_info[:10]):  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10
            if description:
                minutes = int(timestamp // 60)
                seconds = int(timestamp % 60)
                screenshots_summary += f"\n{minutes}:{seconds:02d} - {description[:200]}..."
    
    prompt = f"""
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤–∏–¥–µ–æ "{video_title}".
    
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç:
    {transcript_text}
    {screenshots_summary}
    
    –°–æ–∑–¥–∞–π —Å–ª–µ–¥—É—é—â–µ–µ:
    1. –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
    2. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–∑–∏—Å—ã –∏ –∫–ª—é—á–µ–≤—ã–µ –º—ã—Å–ª–∏ (8-10 –ø—É–Ω–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞)
    3. –í–∏–∑—É–∞–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã (–µ—Å–ª–∏ –±—ã–ª–∏ –ø–æ–∫–∞–∑–∞–Ω—ã –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏, –∫–æ–¥, –¥–µ–º–æ - –æ–ø–∏—à–∏ —á—Ç–æ –∏–º–µ–Ω–Ω–æ)
    4. –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)
    """
    
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": "–¢—ã - –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –≤—ã–¥–µ–ª—è—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏ –∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.1,
            "max_tokens": 2000
        }
        
        response = requests.post(
            "https://api.openai.com/v1/chat/completions", 
            headers=headers, 
            json=payload
        )
        result = response.json()
        
        if "choices" in result and len(result["choices"]) > 0:
            analysis = result["choices"][0]["message"]["content"]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à, –µ—Å–ª–∏ –º–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω
            if cache_manager and video_path:
                cache_manager.save_analysis_cache(video_path, analysis, "basic")
            
            return analysis
        else:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {result}")
            return None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞: {e}")
        return None

def get_image_base64(image_path):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É base64 –¥–ª—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –≤ markdown"""
    try:
        with open(image_path, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
            ext = os.path.splitext(image_path)[1].lstrip('.')
            return f"data:image/{ext};base64,{encoded_string}"
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ base64: {e}")
        return None

def save_chronological_results(video_name, chronological_data, output_dir="results"):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–û–ì–û —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º–∏ –≤ –¥–∏–∞–ª–æ–≥–µ"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    speakers = chronological_data['speakers']
    timeline = chronological_data['timeline']
    structured_content = chronological_data['structured_content']
    report = chronological_data['report']
    
    # –°–æ–∑–¥–∞–µ–º –ï–î–ò–ù–£–Æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ª–∏–Ω–∏—é —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ —Å–æ–±—ã—Ç–∏—è–º–∏
    unified_timeline = []
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–æ–±—ã—Ç–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    for event in timeline:
        unified_timeline.append(event)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Å–æ–±—ã—Ç–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    unified_timeline.sort(key=lambda x: x.timestamp)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
    md_file = f"{output_dir}/{video_name}_INTEGRATED_chronological.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(f"# üé¨ –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ –≤—Å—Ç—Ä–µ—á–∏: {video_name}\n\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É—á–∞—Å—Ç–Ω–∏–∫–∞—Ö
        f.write("## üë• –£—á–∞—Å—Ç–Ω–∏–∫–∏\n\n")
        for speaker_id, speaker in speakers.items():
            name = speaker.name or f"–£—á–∞—Å—Ç–Ω–∏–∫ {speaker_id[-1]}"
            role = speaker.role or "—É—á–∞—Å—Ç–Ω–∏–∫"
            f.write(f"- **{name}** ({role})")
            if speaker.characteristics:
                f.write(f" - {', '.join(speaker.characteristics[:2])}")
            f.write("\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑
        f.write(f"\n## üìã –ö—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑\n\n{report}\n\n")
        
        # –ì–õ–ê–í–ù–ê–Ø –ß–ê–°–¢–¨ - –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–ê–Ø –í–†–ï–ú–ï–ù–ù–ê–Ø –õ–ò–ù–ò–Ø
        f.write("## üéØ –•–†–û–ù–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –•–û–î –í–°–¢–†–ï–ß–ò (—Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º–∏)\n\n")
        f.write("*–í—Å–µ —Å–æ–±—ã—Ç–∏—è —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω—ã –≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ. –°–∫—Ä–∏–Ω—à–æ—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ –Ω–∞ —ç–∫—Ä–∞–Ω–µ –≤ –º–æ–º–µ–Ω—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.*\n\n")
        f.write("---\n\n")
        
        current_topic = "–ù–∞—á–∞–ª–æ –≤—Å—Ç—Ä–µ—á–∏"
        last_speaker = None
        screenshot_counter = 1
        
        for event in unified_timeline:
            minutes = int(event.timestamp // 60)
            seconds = int(event.timestamp % 60)
            timestamp_str = f"{minutes}:{seconds:02d}"
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–º–µ–Ω—ã —Ç–µ–º—ã
            if event.type == 'topic_change':
                current_topic = event.content.get('topic', '–ù–æ–≤–∞—è —Ç–µ–º–∞')
                f.write(f"\n\n### üìå [{timestamp_str}] –¢–ï–ú–ê: {current_topic.upper()}\n\n")
                continue
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
            elif event.type == 'transcript':
                speaker_id = event.content.get('speaker_id', 'unknown')
                speaker_name = event.content.get('speaker_name')
                
                if not speaker_name:
                    speaker_obj = speakers.get(speaker_id)
                    speaker_name = speaker_obj.name if speaker_obj else f"–£—á–∞—Å—Ç–Ω–∏–∫ {speaker_id[-1] if speaker_id != 'unknown' else '1'}"
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
                text = event.content.get('corrected_text', event.content.get('text', ''))
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≥—Ä—É–ø–ø—ã, –µ—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Ä–µ–ø–ª–∏–∫–∞ –≤ –≥—Ä—É–ø–ø–µ
                overall_context = event.content.get('overall_context')
                if overall_context:
                    f.write(f"\nüí° **–ö–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–º–µ–Ω—Ç–∞:** *{overall_context}*\n\n")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–º–µ–Ω—É –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –±–æ–ª–µ–µ –∑–∞–º–µ—Ç–Ω–æ
                if speaker_name != last_speaker:
                    f.write(f"\n**[{timestamp_str}] {speaker_name}:** {text}")
                    last_speaker = speaker_name
                else:
                    f.write(f" {text}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è
                context_explanation = event.content.get('context_explanation')
                if context_explanation:
                    f.write(f" *[{context_explanation}]*")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                visual_reference = event.content.get('visual_reference')
                if visual_reference:
                    f.write(f" üëÅÔ∏è *{visual_reference}*")
                
                f.write("\n")
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ - –í–°–¢–†–ê–ò–í–ê–ï–ú –ü–†–Ø–ú–û –í –î–ò–ê–õ–û–ì
            elif event.type == 'screenshot':
                reason = event.content.get('reason', '–í–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç')
                description = event.content.get('description', '')
                image_path = event.content.get('path', '')
                
                f.write(f"\n\nüì∏ **[{timestamp_str}] –°–ö–†–ò–ù–®–û–¢ #{screenshot_counter}: {reason}**\n\n")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ü–†–Ø–ú–û –í –ü–û–¢–û–ö
                if image_path and os.path.exists(image_path):
                    image_base64 = get_image_base64(image_path)
                    if image_base64:
                        f.write(f"![–°–∫—Ä–∏–Ω—à–æ—Ç {screenshot_counter}]({image_base64})\n\n")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –Ω–∞ —ç–∫—Ä–∞–Ω–µ
                if description:
                    f.write(f"*–ù–∞ —ç–∫—Ä–∞–Ω–µ: {description}*\n\n")
                else:
                    f.write(f"*–í–∏–∑—É–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –Ω–∞ —ç–∫—Ä–∞–Ω–µ –≤ –º–æ–º–µ–Ω—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞*\n\n")
                
                screenshot_counter += 1
                f.write("---\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        f.write(f"\n\n## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Å—Ç—Ä–µ—á–∏\n\n")
        total_duration = max([e.timestamp for e in unified_timeline]) if unified_timeline else 0
        total_screenshots = len([e for e in unified_timeline if e.type == 'screenshot'])
        total_topics = len([e for e in unified_timeline if e.type == 'topic_change'])
        
        f.write(f"- **–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {int(total_duration // 60)}:{int(total_duration % 60):02d}\n")
        f.write(f"- **–£—á–∞—Å—Ç–Ω–∏–∫–æ–≤:** {len(speakers)}\n")
        f.write(f"- **–°–∫—Ä–∏–Ω—à–æ—Ç–æ–≤:** {total_screenshots}\n")
        f.write(f"- **–°–º–µ–Ω —Ç–µ–º—ã:** {total_topics}\n")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º
        f.write(f"\n### –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤:\n")
        for speaker_id, speaker in speakers.items():
            name = speaker.name or f"–£—á–∞—Å—Ç–Ω–∏–∫ {speaker_id[-1]}"
            segments_count = len(speaker.voice_segments) if speaker.voice_segments else 0
            f.write(f"- **{name}:** {segments_count} —Ä–µ–ø–ª–∏–∫\n")
    
    print(f"üéØ –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–´–ô —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {md_file}")
    
    # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    json_file = f"{output_dir}/{video_name}_integrated_data.json"
    json_data = {
        "video_name": video_name,
        "total_duration": max([e.timestamp for e in unified_timeline]) if unified_timeline else 0,
        "speakers": {
            speaker_id: {
                "name": speaker.name,
                "role": speaker.role,
                "characteristics": speaker.characteristics,
                "segments_count": len(speaker.voice_segments) if speaker.voice_segments else 0
            }
            for speaker_id, speaker in speakers.items()
        },
        "timeline_events_count": {
            "total": len(unified_timeline),
            "transcript": len([e for e in unified_timeline if e.type == 'transcript']),
            "screenshots": len([e for e in unified_timeline if e.type == 'screenshot']),
            "topic_changes": len([e for e in unified_timeline if e.type == 'topic_change'])
        },
        "report": report
    }
    
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=4)
    
    return md_file, json_file

def save_results(video_name, transcript_segments, full_transcript, analysis, screenshots=None, output_dir="results"):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
    md_file = f"{output_dir}/{video_name}_analysis.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(f"# –ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ: {video_name}\n\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑
        if analysis:
            f.write(analysis)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if screenshots and len(screenshots) > 0:
            f.write("\n\n## –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –≤–∏–¥–µ–æ\n\n")
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –ø–æ —Ç–∏–ø—É
            ai_screenshots = [s for s in screenshots if s[3] != "periodic"]
            periodic_screenshots = [s for s in screenshots if s[3] == "periodic"]
            
            if ai_screenshots:
                f.write("### ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –≤–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã\n\n")
                for i, (image_path, timestamp, description, reason) in enumerate(ai_screenshots):
                    minutes = timestamp // 60
                    seconds = timestamp % 60
                    f.write(f"#### {minutes}:{seconds:02d} - {reason}\n\n")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ base64
                    image_base64 = get_image_base64(image_path)
                    if image_base64:
                        f.write(f"![–°–∫—Ä–∏–Ω—à–æ—Ç]({image_base64})\n\n")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
                    if description:
                        f.write(f"{description}\n\n")
                    
                    f.write("---\n\n")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –≤ –∫–æ–Ω—Ü–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if periodic_screenshots and len(ai_screenshots) < 5:
                f.write("### üì∏ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã\n\n")
                for i, (image_path, timestamp, description, _) in enumerate(periodic_screenshots[:5]):
                    minutes = timestamp // 60
                    seconds = timestamp % 60
                    f.write(f"#### {minutes}:{seconds:02d}\n\n")
                    
                    image_base64 = get_image_base64(image_path)
                    if image_base64:
                        f.write(f"![–°–∫—Ä–∏–Ω—à–æ—Ç]({image_base64})\n\n")
                    
                    if description:
                        f.write(f"{description}\n\n")
                    
                    f.write("---\n\n")
        
        f.write("\n\n## –ü–æ–ª–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç\n\n")
        f.write(full_transcript)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        if transcript_segments:
            f.write("\n\n## –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏\n\n")
            for segment in transcript_segments:
                start_time = segment['start']
                minutes = int(start_time // 60)
                seconds = int(start_time % 60)
                f.write(f"**{minutes}:{seconds:02d}** - {segment['text']}\n\n")
    
    print(f"–ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ Markdown —Ñ–∞–π–ª: {md_file}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
    json_file = f"{output_dir}/{video_name}_analysis.json"
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è JSON
    screenshots_data = []
    if screenshots:
        for image_path, timestamp, description, reason in screenshots:
            screenshots_data.append({
                "image_path": image_path,
                "timestamp": timestamp,
                "description": description,
                "reason": reason
            })
    
    results = {
        "video_name": video_name,
        "full_transcript": full_transcript,
        "transcript_segments": transcript_segments,
        "analysis": analysis,
        "screenshots": screenshots_data
    }
    
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
        
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON: {json_file}")
    
    return md_file, json_file

def main():
    load_dotenv()
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  python local_video_analyzer.py <–ø—É—Ç—å_–∫_–≤–∏–¥–µ–æ> [–æ–ø—Ü–∏–∏]")
        print("\n–û–ø—Ü–∏–∏:")
        print("  --whisper-model MODEL      –ú–æ–¥–µ–ª—å Whisper (tiny, base, small, medium, large)")
        print("  --screenshot-interval N    –ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 30)")
        print("  --no-screenshots          –ù–µ –∏–∑–≤–ª–µ–∫–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç—ã")
        print("  --smart-screenshots       –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ò–ò –¥–ª—è —É–º–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤")
        print("  --screenshot-mode MODE    –†–µ–∂–∏–º —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: 'periodic', 'smart', 'transcript', 'both' (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: transcript)")
        print("  --chronological           –°–æ–∑–¥–∞—Ç—å —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç —Å —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π")
        print("  --output DIR              –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: results)")
        print("  --clear-cache             –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ –≤–∏–¥–µ–æ")
        print("  --force-refresh           –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ (–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—ç—à)")
        print("  --cache-status            –ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç—É—Å –∫—ç—à–∞ –∏ –≤—ã–π—Ç–∏")
        print("\n–ü—Ä–∏–º–µ—Ä:")
        print("  python local_video_analyzer.py video.mp4 --whisper-model base --chronological")
        sys.exit(1)
    
    video_path = sys.argv[1]
    
    # –ü–∞—Ä—Å–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    whisper_model = "base"
    screenshot_interval = 30
    extract_screenshots_flag = True
    screenshot_mode = "transcript"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
    chronological_mode = False
    output_dir = "results"
    clear_cache = False
    force_refresh = False
    cache_status_only = False
    
    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == "--whisper-model" and i + 1 < len(sys.argv):
            whisper_model = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == "--screenshot-interval" and i + 1 < len(sys.argv):
            screenshot_interval = int(sys.argv[i + 1])
            i += 2
        elif sys.argv[i] == "--no-screenshots":
            extract_screenshots_flag = False
            i += 1
        elif sys.argv[i] == "--smart-screenshots":
            screenshot_mode = "smart"
            i += 1
        elif sys.argv[i] == "--screenshot-mode" and i + 1 < len(sys.argv):
            screenshot_mode = sys.argv[i + 1]
            if screenshot_mode not in ["periodic", "smart", "transcript", "both"]:
                print(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: {screenshot_mode}")
                sys.exit(1)
            i += 2
        elif sys.argv[i] == "--output" and i + 1 < len(sys.argv):
            output_dir = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == "--chronological":
            chronological_mode = True
            i += 1
        elif sys.argv[i] == "--clear-cache":
            clear_cache = True
            i += 1
        elif sys.argv[i] == "--force-refresh":
            force_refresh = True
            i += 1
        elif sys.argv[i] == "--cache-status":
            cache_status_only = True
            i += 1
        else:
            i += 1
    
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: API –∫–ª—é—á OpenAI –Ω–µ –Ω–∞–π–¥–µ–Ω. –ê–Ω–∞–ª–∏–∑ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω.")
        print("–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–±–∞–≤—å—Ç–µ OPENAI_API_KEY –≤ .env —Ñ–∞–π–ª")
        if screenshot_mode in ["smart", "transcript"]:
            screenshot_mode = "periodic"  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –±–µ–∑ API
    
    if not os.path.exists(video_path):
        print(f"–û—à–∏–±–∫–∞: –í–∏–¥–µ–æ—Ñ–∞–π–ª {video_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        sys.exit(1)
    
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    print(f"–ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ: {video_name}")
    print(f"–†–µ–∂–∏–º —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: {screenshot_mode}")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∞
        cache_manager = CacheManager()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –∫—ç—à–∞
        if clear_cache:
            print("üßπ –û—á–∏—â–∞–µ–º –∫—ç—à –¥–ª—è –≤–∏–¥–µ–æ...")
            cache_manager.clear_cache(video_path)
            print("‚úÖ –ö—ç—à –æ—á–∏—â–µ–Ω")
            return
        
        if cache_status_only:
            cache_manager.print_cache_status(video_path)
            return
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –∫—ç—à–∞
        cache_manager.print_cache_status(video_path)
        
        # –ï—Å–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –≤—Ä–µ–º–µ–Ω–Ω–æ "–æ—Ç–∫–ª—é—á–∞–µ–º" –∫—ç—à
        if force_refresh:
            print("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫—ç—à")
            cache_manager = None
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫—ç—à–∞)
        audio_path = extract_audio_from_video(video_path, cache_manager=cache_manager)
        if not audio_path:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∞—É–¥–∏–æ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            sys.exit(1)
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å –ø–æ–º–æ—â—å—é Whisper (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫—ç—à–∞)
        transcript_segments, full_transcript = create_transcript_with_whisper(
            audio_path, whisper_model, video_path=video_path, cache_manager=cache_manager
        )
        if not full_transcript:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            sys.exit(1)
        
        print(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞–Ω. –î–ª–∏–Ω–∞: {len(full_transcript)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        screenshots = []
        if extract_screenshots_flag:
            screenshots_dir = os.path.join(output_dir, f"{video_name}_screenshots")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
            cached_screenshots = cache_manager.get_cached_screenshots(video_path, screenshot_mode)
            if cached_screenshots:
                print(f"üì∏ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã —Ä–µ–∂–∏–º–∞ '{screenshot_mode}'")
                screenshots = [(shot['path'], shot['timestamp'], shot['description'], shot['reason']) 
                              for shot in cached_screenshots]
            else:
                if screenshot_mode == "transcript":
                    print("\nüß† –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤...")
                    extractor = SmartTranscriptExtractor(api_key)
                    screenshots = extractor.extract_screenshots(
                        video_path, screenshots_dir, transcript_segments
                    )
                
                elif screenshot_mode == "smart":
                    print("\nü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ò–ò –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤...")
                    extractor = AdaptiveScreenshotExtractor(api_key)
                    screenshots = extractor.extract_screenshots(
                        video_path, screenshots_dir, transcript_segments
                    )
                
                elif screenshot_mode == "periodic":
                    print(f"\nüì∏ –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –∫–∞–∂–¥—ã–µ {screenshot_interval} —Å–µ–∫—É–Ω–¥...")
                    screenshots = extract_screenshots_traditional(
                        video_path, screenshots_dir, screenshot_interval, api_key
                    )
                
                elif screenshot_mode == "both":
                    print("\nüî¨ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –∞–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ + –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ...")
                    # –°–Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
                    transcript_extractor = SmartTranscriptExtractor(api_key)
                    transcript_screenshots = transcript_extractor.extract_screenshots(
                        video_path, screenshots_dir, transcript_segments
                    )
                    
                    # –ó–∞—Ç–µ–º –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ (—Ä–µ–∂–µ)
                    periodic_screenshots = extract_screenshots_traditional(
                        video_path, screenshots_dir, screenshot_interval * 2, api_key  # –†–µ–∂–µ –¥–ª—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
                    )
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    screenshots = transcript_screenshots
                    for p_shot in periodic_screenshots:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ –±–ª–∏–∑–∫–æ–≥–æ —É–º–Ω–æ–≥–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
                        is_duplicate = any(
                            abs(s[1] - p_shot[1]) < 10  # –í –ø—Ä–µ–¥–µ–ª–∞—Ö 10 —Å–µ–∫—É–Ω–¥
                            for s in transcript_screenshots
                        )
                        if not is_duplicate:
                            screenshots.append(p_shot)
                    
                    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                    screenshots.sort(key=lambda x: x[1])
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –≤ –∫—ç—à –ø–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                if screenshots:
                    cache_manager.save_screenshots_cache(video_path, screenshots, screenshot_mode)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
        analysis = None
        if api_key:
            print("\nüìä –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é GPT...")
            analysis = analyze_content_with_gpt(
                full_transcript, api_key, video_name, screenshots, 
                video_path=video_path, cache_manager=cache_manager
            )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if chronological_mode and CHRONOLOGICAL_AVAILABLE and api_key:
            print("\nüé¨ –°–æ–∑–¥–∞–µ–º —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑...")
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–∏–¥–µ–æ –¥–ª—è —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            video_context = {
                'meeting_type': 'discussion',
                'main_topics': [],
                'visual_content_probability': 0.5
            }
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            screenshots_formatted = []
            if screenshots:
                if isinstance(screenshots[0], dict):
                    # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–∑ AdaptiveScreenshotExtractor
                    screenshots_formatted = [
                        (shot['path'], shot['timestamp'], shot.get('description'), shot.get('decision', {}).get('reason', 'screenshot'))
                        for shot in screenshots
                    ]
                else:
                    # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
                    screenshots_formatted = screenshots
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            processor = ChronologicalTranscriptProcessor(api_key)
            chronological_data = processor.process_video_meeting(
                transcript_segments, screenshots_formatted, video_context
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            chron_md, chron_json = save_chronological_results(video_name, chronological_data, output_dir)
            
            print(f"\n‚úÖ –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≥–æ—Ç–æ–≤!")
            print(f"  üé¨ –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç: {chron_md}")
            print(f"  üìã –ü–æ–¥—Ä–æ–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {chron_json}")
        
        elif chronological_mode and not CHRONOLOGICAL_AVAILABLE:
            print("‚ö†Ô∏è  –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–µ—Ä–µ–∏–º–µ–Ω—É–π—Ç–µ chronological-transcript-processor.py")
        
        elif chronological_mode and not api_key:
            print("‚ö†Ô∏è  –î–ª—è —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω—É–∂–µ–Ω API –∫–ª—é—á OpenAI")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ã—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–µ—Å–ª–∏ –Ω–µ —Ç–æ–ª—å–∫–æ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º)
        if not chronological_mode or not CHRONOLOGICAL_AVAILABLE or not api_key:
            md_file, json_file = save_results(video_name, transcript_segments, full_transcript, analysis, screenshots, output_dir)
        
        print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"  üìÑ Markdown: {md_file}")
        print(f"  üìã JSON: {json_file}")
        if screenshots:
            print(f"  üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç—ã: {len(screenshots)} —Ñ–∞–π–ª–æ–≤")
            if screenshot_mode == "transcript":
                print(f"     üß† –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞–π–¥–µ–Ω–æ {len(screenshots)} –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤")
            elif screenshot_mode == "smart":
                ai_count = len([s for s in screenshots if s[3] != "periodic"])
                print(f"     ü§ñ –ò–ò –æ–ø—Ä–µ–¥–µ–ª–∏–ª {ai_count} –≤–∞–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤")
            elif screenshot_mode == "both":
                transcript_count = len([s for s in screenshots if getattr(s, 'method', 'transcript') == 'transcript'])
                periodic_count = len([s for s in screenshots if s[3] == "periodic"])
                print(f"     üß† –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç: {transcript_count}, üì∏ –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ: {periodic_count}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        metadata = {
            'whisper_model': whisper_model,
            'screenshot_mode': screenshot_mode,
            'chronological_mode': chronological_mode,
            'transcript_length': len(full_transcript),
            'segments_count': len(transcript_segments),
            'screenshots_count': len(screenshots) if screenshots else 0
        }
        cache_manager.save_metadata(video_path, metadata)
        
        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ—Ñ–∞–π–ª (–ù–ï –∏–∑ –∫—ç—à–∞)
        if audio_path and os.path.exists(audio_path) and not audio_path.startswith(cache_manager.cache_dir):
            os.remove(audio_path)
            print("\nüßπ –í—Ä–µ–º–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ—Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω")
        
    except Exception as e:
        print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
