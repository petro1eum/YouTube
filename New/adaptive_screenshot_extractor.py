#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ —Å –æ–±—É—á–µ–Ω–∏–µ–º –Ω–∞ –ª–µ—Ç—É
"""

import os
import json
import cv2
import numpy as np
import base64
from openai import OpenAI
from PIL import Image
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
from collections import deque
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScreenshotDecision:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è –æ —Å–∫—Ä–∏–Ω—à–æ—Ç–µ"""
    capture: bool
    reason: str
    importance: float
    confidence: float
    visual_features: Dict
    context_match: float

@dataclass
class VideoContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –≤–∏–¥–µ–æ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    meeting_type: str
    main_topics: List[str]
    visual_content_probability: float
    recommended_strategy: str
    key_participants: List[str]
    expected_demonstrations: List[str]

class AdaptiveScreenshotExtractor:
    """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = OpenAI(api_key=api_key)
        
        # –ò—Å—Ç–æ—Ä–∏—è —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.decision_history = deque(maxlen=100)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.importance_threshold = 0.6
        self.confidence_threshold = 0.7
        self.min_interval = 3.0  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º–∏
        self.check_interval = 2.0  # –∫–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
        self.weights = {
            "visual_change": 1.0,
            "transcript_relevance": 1.2,
            "content_type": 1.1,
            "demonstration_keywords": 1.5,
            "time_since_last": 0.8
        }
        
        # –ö—ç—à –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.frame_cache = deque(maxlen=10)
        self.analysis_cache = {}
        
    def extract_screenshots(self, video_path: str, output_dir: str, 
                           transcript_segments: List[Dict]) -> List[Tuple]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π"""
        
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ: {video_path}")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–∏–¥–µ–æ
        video_context = self.analyze_video_context(video_path, transcript_segments)
        logger.info(f"–¢–∏–ø –≤—Å—Ç—Ä–µ—á–∏: {video_context.meeting_type}")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.adjust_parameters(video_context)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç—ã —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        screenshots = self.intelligent_extraction(
            video_path, output_dir, transcript_segments, video_context
        )
        
        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        return self.post_process_screenshots(screenshots, video_context)
    
    def analyze_video_context(self, video_path: str, 
                            transcript_segments: List[Dict]) -> VideoContext:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–∏–¥–µ–æ"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –º–∏–Ω—É—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        early_transcript = []
        for segment in transcript_segments:
            if segment['start'] < 300:  # –ø–µ—Ä–≤—ã–µ 5 –º–∏–Ω—É—Ç
                early_transcript.append(segment['text'])
            else:
                break
        
        full_early_text = " ".join(early_transcript)
        
        # –¢–∞–∫–∂–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–¥—Ä–æ–≤ –≤–∏–¥–µ–æ
        visual_summary = self.analyze_video_sample(video_path)
        
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–∏–¥–µ–æ –≤—Å—Ç—Ä–µ—á–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –µ—ë –∫–æ–Ω—Ç–µ–∫—Å—Ç.

–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ø–µ—Ä–≤—ã—Ö –º–∏–Ω—É—Ç:
{full_early_text[:3000]}

–í–∏–∑—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:
{visual_summary}

–û–ø—Ä–µ–¥–µ–ª–∏:
1. meeting_type: —Ç–∏–ø –≤—Å—Ç—Ä–µ—á–∏ (presentation/code_review/discussion/demo/training/workshop)
2. main_topics: –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã (—Å–ø–∏—Å–æ–∫ –∏–∑ 3-5 —Ç–µ–º)
3. visual_content_probability: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (0.0-1.0)
4. recommended_strategy: —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ (frequent/moderate/sparse/contextual)
5. key_participants: –∫–ª—é—á–µ–≤—ã–µ —É—á–∞—Å—Ç–Ω–∏–∫–∏ (–µ—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è)
6. expected_demonstrations: —á—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ –±—É–¥–µ—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è (code/slides/documents/diagrams/ui)

–û—Ç–≤–µ—Ç—å –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ."""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            context_data = json.loads(response.choices[0].message.content)
            
            return VideoContext(
                meeting_type=context_data.get('meeting_type', 'discussion'),
                main_topics=context_data.get('main_topics', []),
                visual_content_probability=context_data.get('visual_content_probability', 0.5),
                recommended_strategy=context_data.get('recommended_strategy', 'moderate'),
                key_participants=context_data.get('key_participants', []),
                expected_demonstrations=context_data.get('expected_demonstrations', [])
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            return VideoContext(
                meeting_type='discussion',
                main_topics=[],
                visual_content_probability=0.5,
                recommended_strategy='moderate',
                key_participants=[],
                expected_demonstrations=[]
            )
    
    def analyze_video_sample(self, video_path: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—Ä–∞–∑—Ü—ã –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # –ë–µ—Ä–µ–º 5 –∫–∞–¥—Ä–æ–≤ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –ø–æ –≤–∏–¥–µ–æ
        sample_positions = np.linspace(0, total_frames - 1, 5, dtype=int)
        
        visual_info = []
        for pos in sample_positions:
            cap.set(cv2.CAP_PROP_POS_FRAMES, pos)
            ret, frame = cap.read()
            if ret:
                info = self.analyze_frame_content(frame)
                visual_info.append(info)
        
        cap.release()
        
        # –°—É–º–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        content_types = [info['content_type'] for info in visual_info]
        most_common = max(set(content_types), key=content_types.count)
        
        return f"–ü—Ä–µ–æ–±–ª–∞–¥–∞—é—â–∏–π —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {most_common}, –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç–∏–ø—ã: {set(content_types)}"
    
    def adjust_parameters(self, video_context: VideoContext):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–∏–¥–µ–æ"""
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ (–£–í–ï–õ–ò–ß–ò–í–ê–ï–ú –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏)
        strategy_intervals = {
            'frequent': 3.0,      # –±—ã–ª–æ 1.0
            'moderate': 5.0,      # –±—ã–ª–æ 2.0  
            'sparse': 10.0,       # –±—ã–ª–æ 5.0
            'contextual': 4.0     # –±—ã–ª–æ 1.5
        }
        self.check_interval = strategy_intervals.get(
            video_context.recommended_strategy, 5.0  # –±—ã–ª–æ 2.0
        )
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø–æ—Ä–æ–≥–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
        if video_context.meeting_type in ['presentation', 'demo', 'training']:
            self.importance_threshold = 0.6  # –±—ã–ª–æ 0.5 - –ø–æ–≤—ã—à–∞–µ–º –ø–æ—Ä–æ–≥
            self.weights['content_type'] = 1.3
        elif video_context.meeting_type == 'code_review':
            self.importance_threshold = 0.65  # –±—ã–ª–æ 0.55
            self.weights['visual_change'] = 1.2
            self.weights['demonstration_keywords'] = 1.6
        else:  # discussion
            self.importance_threshold = 0.8  # –±—ã–ª–æ 0.7 - –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ–º
            self.weights['transcript_relevance'] = 1.4
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if video_context.visual_content_probability > 0.7:
            self.min_interval = 5.0  # –±—ã–ª–æ 2.0
        elif video_context.visual_content_probability < 0.3:
            self.min_interval = 10.0  # –±—ã–ª–æ 5.0
        else:
            self.min_interval = 7.0   # –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–π —Å–ª—É—á–∞–π
        
        logger.info(f"–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: interval={self.check_interval}, "
                   f"threshold={self.importance_threshold}, min_interval={self.min_interval}")
    
    def intelligent_extraction(self, video_path: str, output_dir: str,
                             transcript_segments: List[Dict],
                             video_context: VideoContext) -> List[Tuple]:
        """–£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        logger.info(f"–í–∏–¥–µ–æ: {duration:.1f} —Å–µ–∫, {fps:.1f} FPS")
        
        screenshots = []
        frame_count = 0
        last_check = -self.check_interval
        last_screenshot_time = -self.min_interval
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        state = {
            "last_content_type": None,
            "demonstration_mode": False,
            "important_section": False,
            "recent_screenshots": deque(maxlen=5),
            "scene_stability": 0,
            "transcript_buffer": deque(maxlen=10)
        }
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        progress_interval = int(total_frames / 20)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_time = frame_count / fps
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if frame_count % progress_interval == 0:
                progress = (frame_count / total_frames) * 100
                logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –∫–∞–¥—Ä–æ–≤
            self.frame_cache.append((frame.copy(), current_time))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
            if current_time - last_check >= self.check_interval:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
                self.update_transcript_state(state, transcript_segments, current_time)
                
                # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ
                decision = self.make_screenshot_decision(
                    frame, current_time, transcript_segments,
                    video_context, state
                )
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                self.decision_history.append({
                    'time': current_time,
                    'decision': decision,
                    'state': state.copy()
                })
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç
                if decision.capture and (current_time - last_screenshot_time) >= self.min_interval:
                    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –∫–∞–¥—Ä –∏–∑ –∫—ç—à–∞
                    best_frame, best_time = self.select_best_frame(decision)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç
                    screenshot_path = self.save_screenshot(
                        best_frame, best_time, output_dir, decision
                    )
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                    description = self.generate_contextual_description(
                        screenshot_path, best_time,
                        transcript_segments, video_context,
                        decision
                    )
                    
                    screenshots.append({
                        'path': screenshot_path,
                        'timestamp': best_time,
                        'description': description,
                        'decision': decision,
                        'context': video_context.meeting_type
                    })
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                    state['recent_screenshots'].append(best_time)
                    last_screenshot_time = best_time
                    
                    # –û–±—É—á–∞–µ–º—Å—è –Ω–∞ —Ä–µ—à–µ–Ω–∏–∏
                    self.learn_from_decision(decision, True)
                    
                    logger.info(f"üì∏ {best_time:.1f}—Å: {decision.reason} "
                              f"(–≤–∞–∂–Ω–æ—Å—Ç—å: {decision.importance:.2f})")
                
                last_check = current_time
            
            frame_count += 1
        
        cap.release()
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(screenshots)} —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤")
        
        return screenshots
    
    def make_screenshot_decision(self, frame: np.ndarray, current_time: float,
                               transcript_segments: List[Dict],
                               video_context: VideoContext,
                               state: Dict) -> ScreenshotDecision:
        """–ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞"""
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        visual_features = self.analyze_frame_content(frame)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
        transcript_context = self.get_transcript_context(
            transcript_segments, current_time, window=10
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –≤–∞–∂–Ω–æ—Å—Ç–∏
        factors = self.calculate_importance_factors(
            visual_features, transcript_context, state, 
            current_time, video_context
        )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è
        prompt = f"""–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –º–æ–º–µ–Ω—Ç –≤–∏–¥–µ–æ –∏ —Ä–µ—à–∏, –Ω—É–∂–µ–Ω –ª–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç.

–í—Ä–µ–º—è: {current_time:.1f} —Å–µ–∫
–¢–∏–ø –≤—Å—Ç—Ä–µ—á–∏: {video_context.meeting_type}

–í–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
- –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {visual_features['content_type']}
- –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ü–µ–Ω—ã: {visual_features['scene_change']:.2f}
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞: {visual_features['text_amount']}
- –°–ª–æ–∂–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {visual_features['complexity']:.2f}

–ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞:
{transcript_context['text'][:500]}

–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {transcript_context['keywords']}
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è: {state.get('demonstration_mode', False)}

–§–∞–∫—Ç–æ—Ä—ã –≤–∞–∂–Ω–æ—Å—Ç–∏:
{json.dumps(factors, indent=2)}

–ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–∫—Ä–∏–Ω—à–æ—Ç: {state['recent_screenshots'][-1] if state['recent_screenshots'] else '–Ω–µ—Ç'}

–ù—É–∂–µ–Ω –ª–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç? –û—Ç–≤–µ—Ç—å –≤ JSON:
{{
    "capture": true/false,
    "reason": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ",
    "importance": 0.0-1.0,
    "confidence": 0.0-1.0
}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.2
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞
            weighted_importance = self.apply_weights(result['importance'], factors)
            
            return ScreenshotDecision(
                capture=result['capture'] and weighted_importance >= self.importance_threshold,
                reason=result['reason'],
                importance=weighted_importance,
                confidence=result.get('confidence', 0.8),
                visual_features=visual_features,
                context_match=factors.get('context_match', 0.5)
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–Ω—è—Ç–∏–∏ —Ä–µ—à–µ–Ω–∏—è: {e}")
            # Fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫—É
            return self.heuristic_decision(visual_features, transcript_context, state)
    
    def analyze_frame_content(self, frame: np.ndarray) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–∞–¥—Ä–∞"""
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –∫—Ä–∞–µ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        # –ê–Ω–∞–ª–∏–∑ —Ü–≤–µ—Ç–æ–≤
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        saturation_mean = np.mean(hsv[:, :, 1])
        value_mean = np.mean(hsv[:, :, 2])
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_type = self.detect_content_type(frame, edge_density, saturation_mean)
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–∞–¥—Ä)
        scene_change = 0.0
        if len(self.frame_cache) > 1:
            prev_frame = self.frame_cache[-2][0]
            scene_change = self.calculate_scene_change(prev_frame, frame)
        
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞
        text_amount = self.estimate_text_amount(edges, edge_density)
        
        return {
            'content_type': content_type,
            'scene_change': scene_change,
            'text_amount': text_amount,
            'complexity': edge_density,
            'saturation': saturation_mean,
            'brightness': value_mean
        }
    
    def detect_content_type(self, frame: np.ndarray, edge_density: float, 
                          saturation: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ —ç–∫—Ä–∞–Ω–µ"""
        
        # –ü—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
        if edge_density > 0.15:
            if saturation < 30:
                return "code_editor"
            else:
                return "text_document"
        elif edge_density > 0.08:
            if saturation > 100:
                return "presentation_slide"
            else:
                return "diagram"
        elif edge_density < 0.05:
            if saturation > 50:
                return "video_share"
            else:
                return "blank_screen"
        else:
            return "mixed_content"
    
    def calculate_scene_change(self, prev_frame: np.ndarray, 
                              curr_frame: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–µ–ø–µ–Ω—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫
        # 1. –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–Ω–∏—Ü–∞
        diff = cv2.absdiff(prev_frame, curr_frame)
        mean_diff = np.mean(diff) / 255.0
        
        # 2. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
        hist1 = cv2.calcHist([prev_frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
        hist2 = cv2.calcHist([curr_frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
        hist1 = cv2.normalize(hist1, hist1).flatten()
        hist2 = cv2.normalize(hist2, hist2).flatten()
        hist_corr = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
        scene_change = mean_diff * 0.5 + (1 - hist_corr) * 0.5
        
        return min(scene_change, 1.0)
    
    def estimate_text_amount(self, edges: np.ndarray, edge_density: float) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
        
        if edge_density > 0.12:
            return "high"
        elif edge_density > 0.06:
            return "medium"
        elif edge_density > 0.02:
            return "low"
        else:
            return "none"
    
    def get_transcript_context(self, transcript_segments: List[Dict],
                             current_time: float, window: float = 10) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –≤–æ–∫—Ä—É–≥ —Ç–µ–∫—É—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏"""
        
        relevant_segments = []
        keywords = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≤–∞–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤
        important_keywords = [
            '–ø–æ–∫–∞–∂—É', '–ø–æ–∫–∞–∑—ã–≤–∞—é', '—Å–º–æ—Ç—Ä–∏—Ç–µ', '–≤–∏–¥–∏—Ç–µ', '–≤–æ—Ç –∑–¥–µ—Å—å',
            '–Ω–∞ —ç–∫—Ä–∞–Ω–µ', '–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è', '–ø—Ä–∏–º–µ—Ä', '–¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º',
            '–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ', '–≤–∞–∂–Ω–æ', '–≥–ª–∞–≤–Ω–æ–µ', '–∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç',
            '–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è', '—Å–ª–∞–π–¥', '–∫–æ–¥', '–¥–∏–∞–≥—Ä–∞–º–º–∞', '—Å—Ö–µ–º–∞',
            '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–∏—Ç–æ–≥', '–≤—ã–≤–æ–¥', '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ'
        ]
        
        for segment in transcript_segments:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–ø–∞–¥–∞–µ—Ç –ª–∏ —Å–µ–≥–º–µ–Ω—Ç –≤ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ
            if (segment['start'] >= current_time - window and 
                segment['start'] <= current_time + window/2):
                relevant_segments.append(segment)
                
                # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                text_lower = segment['text'].lower()
                for keyword in important_keywords:
                    if keyword in text_lower:
                        keywords.append(keyword)
        
        text = ' '.join([s['text'] for s in relevant_segments])
        
        return {
            'text': text,
            'segments': relevant_segments,
            'keywords': list(set(keywords)),
            'has_demonstration_keywords': len(keywords) > 0
        }
    
    def calculate_importance_factors(self, visual_features: Dict,
                                   transcript_context: Dict,
                                   state: Dict, current_time: float,
                                   video_context: VideoContext) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –≤–∞–∂–Ω–æ—Å—Ç–∏"""
        
        factors = {}
        
        # –í–∏–∑—É–∞–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        factors['visual_change'] = visual_features['scene_change']
        factors['content_relevance'] = 1.0 if visual_features['content_type'] in [
            'presentation_slide', 'code_editor', 'diagram'
        ] else 0.5
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Ñ–∞–∫—Ç–æ—Ä—ã
        factors['has_keywords'] = 1.0 if transcript_context['has_demonstration_keywords'] else 0.0
        factors['transcript_density'] = min(len(transcript_context['text']) / 500, 1.0)
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        if state['recent_screenshots']:
            time_since_last = current_time - state['recent_screenshots'][-1]
            factors['time_factor'] = min(time_since_last / 30, 1.0)  # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 30 —Å–µ–∫
        else:
            factors['time_factor'] = 1.0
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
        factors['context_match'] = self.calculate_context_match(
            visual_features, video_context
        )
        
        # –§–∞–∫—Ç–æ—Ä—ã —Å–æ—Å—Ç–æ—è–Ω–∏—è
        factors['demonstration_mode'] = 1.0 if state.get('demonstration_mode') else 0.3
        
        return factors
    
    def calculate_context_match(self, visual_features: Dict,
                              video_context: VideoContext) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ–∂–∏–¥–∞–Ω–∏—è–º"""
        
        content_type = visual_features['content_type']
        expected = video_context.expected_demonstrations
        
        match_score = 0.0
        
        if 'code' in expected and content_type == 'code_editor':
            match_score = 1.0
        elif 'slides' in expected and content_type == 'presentation_slide':
            match_score = 1.0
        elif 'documents' in expected and content_type == 'text_document':
            match_score = 0.9
        elif 'diagrams' in expected and content_type == 'diagram':
            match_score = 0.95
        elif 'ui' in expected and content_type in ['mixed_content', 'video_share']:
            match_score = 0.8
        else:
            match_score = 0.3
        
        return match_score
    
    def apply_weights(self, base_importance: float, factors: Dict) -> float:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤–µ—Å–∞ –∫ —Ñ–∞–∫—Ç–æ—Ä–∞–º –≤–∞–∂–Ω–æ—Å—Ç–∏"""
        
        weighted_sum = 0.0
        weight_sum = 0.0
        
        for factor_name, factor_value in factors.items():
            if factor_name in self.weights:
                weight = self.weights[factor_name]
                weighted_sum += factor_value * weight
                weight_sum += weight
        
        if weight_sum > 0:
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –≤–∞–∂–Ω–æ—Å—Ç—å —Å –≤–∑–≤–µ—à–µ–Ω–Ω—ã–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏
            weighted_importance = (base_importance * 0.6 + 
                                 (weighted_sum / weight_sum) * 0.4)
        else:
            weighted_importance = base_importance
        
        return min(weighted_importance, 1.0)
    
    def heuristic_decision(self, visual_features: Dict,
                         transcript_context: Dict,
                         state: Dict) -> ScreenshotDecision:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∫–∞–∫ fallback"""
        
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        capture = False
        reason = "no significant change"
        importance = 0.3
        
        if visual_features['scene_change'] > 0.5:
            capture = True
            reason = "significant scene change"
            importance = 0.7
        elif transcript_context['has_demonstration_keywords']:
            capture = True
            reason = "demonstration keywords detected"
            importance = 0.8
        elif visual_features['content_type'] in ['presentation_slide', 'code_editor']:
            if visual_features['scene_change'] > 0.2:
                capture = True
                reason = f"new {visual_features['content_type']}"
                importance = 0.75
        
        return ScreenshotDecision(
            capture=capture,
            reason=reason,
            importance=importance,
            confidence=0.6,
            visual_features=visual_features,
            context_match=0.5
        )
    
    def update_transcript_state(self, state: Dict, transcript_segments: List[Dict],
                              current_time: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞"""
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –±—É—Ñ–µ—Ä —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
        recent_segments = []
        for segment in transcript_segments:
            if segment['start'] >= current_time - 20 and segment['start'] <= current_time:
                recent_segments.append(segment['text'])
        
        state['transcript_buffer'] = deque(recent_segments[-10:], maxlen=10)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∂–∏–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        demo_keywords = ['–ø–æ–∫–∞–∂—É', '–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è', '–ø—Ä–∏–º–µ—Ä', '—Å–º–æ—Ç—Ä–∏—Ç–µ', '–Ω–∞ —ç–∫—Ä–∞–Ω–µ']
        recent_text = ' '.join(state['transcript_buffer']).lower()
        
        state['demonstration_mode'] = any(kw in recent_text for kw in demo_keywords)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å —Å–µ–∫—Ü–∏–∏
        importance_keywords = ['–≤–∞–∂–Ω–æ', '–∫–ª—é—á–µ–≤–æ–π', '–≥–ª–∞–≤–Ω–æ–µ', '–æ—Å–Ω–æ–≤–Ω–æ–π', '–∫—Ä–∏—Ç–∏—á–Ω–æ']
        state['important_section'] = any(kw in recent_text for kw in importance_keywords)
    
    def select_best_frame(self, decision: ScreenshotDecision) -> Tuple[np.ndarray, float]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –∫–∞–¥—Ä –∏–∑ –∫—ç—à–∞"""
        
        if not self.frame_cache:
            return None, 0.0
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–∞–¥—Ä
        if len(self.frame_cache) == 1:
            return self.frame_cache[0]
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤
        best_score = -1
        best_frame = None
        best_time = 0
        
        for frame, timestamp in self.frame_cache:
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–∞
            score = self.evaluate_frame_quality(frame)
            
            # –£—á–∏—Ç—ã–≤–∞–µ–º –±–ª–∏–∑–æ—Å—Ç—å –∫ –º–æ–º–µ–Ω—Ç—É —Ä–µ—à–µ–Ω–∏—è
            if decision.visual_features.get('scene_change', 0) > 0.3:
                # –î–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ü–µ–Ω—ã –±–µ—Ä–µ–º –±–æ–ª–µ–µ –ø–æ–∑–¥–Ω–∏–π –∫–∞–¥—Ä
                time_weight = timestamp / self.frame_cache[-1][1]
            else:
                # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω –±–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–∏–π –∫–∞–¥—Ä
                time_weight = 1.0 - abs(0.5 - timestamp / self.frame_cache[-1][1])
            
            final_score = score * 0.7 + time_weight * 0.3
            
            if final_score > best_score:
                best_score = final_score
                best_frame = frame
                best_time = timestamp
        
        return best_frame, best_time
    
    def evaluate_frame_quality(self, frame: np.ndarray) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–∞"""
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑–º—ã—Ç–∏–µ
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        blur_score = min(laplacian_var / 1000, 1.0)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —è—Ä–∫–æ—Å—Ç—å
        brightness = np.mean(gray) / 255
        brightness_score = 1.0 - abs(brightness - 0.5) * 2
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω—Ç—Ä–∞—Å—Ç
        contrast = gray.std() / 128
        contrast_score = min(contrast, 1.0)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        quality = blur_score * 0.5 + brightness_score * 0.25 + contrast_score * 0.25
        
        return quality
    
    def save_screenshot(self, frame: np.ndarray, timestamp: float,
                       output_dir: str, decision: ScreenshotDecision) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        minutes = int(timestamp // 60)
        seconds = int(timestamp % 60)
        milliseconds = int((timestamp % 1) * 1000)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏—á–∏–Ω–µ –≤ –∏–º—è —Ñ–∞–π–ª–∞
        reason_short = decision.reason.replace(' ', '_')[:20]
        filename = f"screenshot_{minutes:02d}m{seconds:02d}s{milliseconds:03d}ms_{reason_short}.jpg"
        filepath = os.path.join(output_dir, filename)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –≤—ã—Å–æ–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º
        cv2.imwrite(filepath, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'timestamp': timestamp,
            'decision': {
                'reason': decision.reason,
                'importance': decision.importance,
                'confidence': decision.confidence
            },
            'visual_features': decision.visual_features
        }
        
        metadata_path = filepath.replace('.jpg', '_metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        return filepath
    
    def generate_contextual_description(self, screenshot_path: str,
                                      timestamp: float,
                                      transcript_segments: List[Dict],
                                      video_context: VideoContext,
                                      decision: ScreenshotDecision) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞"""
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
        transcript_context = self.get_transcript_context(
            transcript_segments, timestamp, window=15
        )
        
        # –ö–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è GPT-4V
        with open(screenshot_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        prompt = f"""–û–ø–∏—à–∏ —ç—Ç–æ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç –∏–∑ {video_context.meeting_type} –≤—Å—Ç—Ä–µ—á–∏.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
- –í—Ä–µ–º—è: {timestamp:.1f} —Å–µ–∫ ({int(timestamp//60)}:{int(timestamp%60):02d})
- –ü—Ä–∏—á–∏–Ω–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞: {decision.reason}
- –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {decision.visual_features.get('content_type', 'unknown')}
- –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –≤—Å—Ç—Ä–µ—á–∏: {', '.join(video_context.main_topics[:3])}

–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –æ–∫–æ–ª–æ —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞:
{transcript_context['text'][:800]}

–°–æ–∑–¥–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ:
1. –û–ø–∏—Å—ã–≤–∞–µ—Ç —á—Ç–æ –≤–∏–¥–Ω–æ –Ω–∞ —ç–∫—Ä–∞–Ω–µ
2. –°–≤—è–∑—ã–≤–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
3. –í—ã–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
4. –û–±—ä—è—Å–Ω—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞

–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º."""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}",
                                "detail": "low"
                            }
                        }
                    ]
                }],
                max_tokens=300,
                temperature=0.3
            )
            
            description = response.choices[0].message.content.strip()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if decision.importance > 0.8:
                description = f"‚≠ê –í–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç: {description}"
            elif decision.visual_features.get('content_type') == 'code_editor':
                description = f"üíª –ö–æ–¥: {description}"
            elif decision.visual_features.get('content_type') == 'presentation_slide':
                description = f"üìä –°–ª–∞–π–¥: {description}"
            
            return description
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return f"–°–∫—Ä–∏–Ω—à–æ—Ç –≤ {timestamp:.1f}—Å - {decision.reason}"
    
    def learn_from_decision(self, decision: ScreenshotDecision, was_captured: bool):
        """–û–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–Ω—è—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è"""
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é —Ä–µ—à–µ–Ω–∏–π
        if len(self.decision_history) < 10:
            return
        
        # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º —Ä–µ—à–µ–Ω–∏—è–º
        recent_decisions = list(self.decision_history)[-20:]
        capture_rate = sum(1 for d in recent_decisions if d['decision'].capture) / len(recent_decisions)
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø–æ—Ä–æ–≥–∏
        if capture_rate > 0.7:  # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
            self.importance_threshold = min(self.importance_threshold * 1.05, 0.9)
            logger.debug(f"–ü–æ–≤—ã—à–∞–µ–º –ø–æ—Ä–æ–≥ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–æ {self.importance_threshold:.2f}")
        elif capture_rate < 0.2:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
            self.importance_threshold = max(self.importance_threshold * 0.95, 0.4)
            logger.debug(f"–ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–æ {self.importance_threshold:.2f}")
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π
        if was_captured and decision.importance > 0.7:
            # –£—Å–∏–ª–∏–≤–∞–µ–º —Ñ–∞–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–µ–ª–∏ –∫ —Ö–æ—Ä–æ—à–µ–º—É —Ä–µ—à–µ–Ω–∏—é
            for factor_name in ['visual_change', 'transcript_relevance', 'content_type']:
                if factor_name in self.weights:
                    self.weights[factor_name] *= 1.02
                    self.weights[factor_name] = min(self.weights[factor_name], 2.0)
    
    def post_process_screenshots(self, screenshots: List[Dict],
                               video_context: VideoContext) -> List[Tuple]:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        
        if not screenshots:
            return []
        
        logger.info("–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤...")
        
        # –£–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –±–ª–∏–∑–∫–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        filtered = []
        last_time = -self.min_interval
        
        for screenshot in screenshots:
            if screenshot['timestamp'] - last_time >= self.min_interval * 0.8:
                filtered.append(screenshot)
                last_time = screenshot['timestamp']
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤–∞–∂–Ω–µ–µ –ª–∏ —ç—Ç–æ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
                if (screenshot['decision'].importance > 
                    filtered[-1]['decision'].importance * 1.2):
                    # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π
                    filtered[-1] = screenshot
                    logger.debug(f"–ó–∞–º–µ–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –Ω–∞ –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–π: "
                               f"{screenshot['timestamp']:.1f}—Å")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        if len(filtered) > 5:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            timestamps = [s['timestamp'] for s in filtered]
            intervals = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]
            avg_interval = np.mean(intervals)
            std_interval = np.std(intervals)
            
            logger.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: avg={avg_interval:.1f}—Å, "
                       f"std={std_interval:.1f}—Å")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        result = []
        for screenshot in filtered:
            result.append((
                screenshot['path'],
                screenshot['timestamp'],
                screenshot['description'],
                screenshot['decision'].reason
            ))
        
        logger.info(f"–ü–æ—Å–ª–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(result)} —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤")
        
        return result

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –æ—Å–Ω–æ–≤–Ω—ã–º —Å–∫—Ä–∏–ø—Ç–æ–º

def integrate_adaptive_extractor(video_path: str, output_dir: str,
                               transcript_segments: List[Dict],
                               api_key: str) -> List[Tuple]:
    """–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –≤ –æ—Å–Ω–æ–≤–Ω–æ–π pipeline"""
    
    extractor = AdaptiveScreenshotExtractor(api_key)
    screenshots = extractor.extract_screenshots(
        video_path, output_dir, transcript_segments
    )
    
    return screenshots

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python adaptive_screenshot_extractor.py <video_path>")
        sys.exit(1)
    
    video_path = sys.argv[1]
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        print("–û—à–∏–±–∫–∞: –ù–µ–æ–±—Ö–æ–¥–∏–º OPENAI_API_KEY")
        sys.exit(1)
    
    # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
    dummy_transcript = [
        {"text": "–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—é", "start": 10.0, "duration": 3.0},
        {"text": "–í–æ—Ç –∑–¥–µ—Å—å –ø–æ–∫–∞–∑–∞–Ω –≤–∞–∂–Ω—ã–π –∫–æ–¥", "start": 30.0, "duration": 4.0},
    ]
    
    extractor = AdaptiveScreenshotExtractor(api_key)
    screenshots = extractor.extract_screenshots(
        video_path, "output", dummy_transcript
    )
    
    print(f"\n–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(screenshots)} —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤")
    for i, (path, time, desc, reason) in enumerate(screenshots):
        print(f"{i+1}. {time:.1f}—Å - {reason}")
        print(f"   {desc}\n")
