#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
—Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞
"""

import os
import json
import re
import base64
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import numpy as np
from openai import OpenAI
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Speaker:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É—á–∞—Å—Ç–Ω–∏–∫–µ"""
    id: str
    name: Optional[str] = None
    role: Optional[str] = None
    characteristics: List[str] = None
    voice_segments: List[Tuple[float, float]] = None

@dataclass
class TranscriptSegment:
    """–°–µ–≥–º–µ–Ω—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
    start: float
    end: float
    text: str
    speaker_id: Optional[str] = None
    corrected_text: Optional[str] = None
    context_notes: Optional[str] = None
    related_screenshot: Optional[str] = None
    confidence: float = 1.0

@dataclass
class TimelineEvent:
    """–°–æ–±—ã—Ç–∏–µ –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏"""
    timestamp: float
    type: str  # 'transcript', 'screenshot', 'topic_change', 'speaker_change'
    content: Dict
    importance: float = 0.5

class ChronologicalTranscriptProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = OpenAI(api_key=api_key)
        self.speakers = {}
        self.timeline_events = []
        self.topics = []
        self.context_buffer = []
        
    def process_video_meeting(self, transcript_segments: List[Dict], 
                            screenshots: List[Tuple],
                            video_context: Dict) -> Dict:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ –≤—Å—Ç—Ä–µ—á–∏"""
        
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞...")
        
        # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
        speakers = self.identify_speakers(transcript_segments)
        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {len(speakers)}")
        
        # 2. –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ª–∏–Ω–∏—é —Å–æ–±—ã—Ç–∏–π
        timeline = self.create_timeline(transcript_segments, screenshots)
        
        # 3. –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        corrected_timeline = self.correct_transcript_with_context(
            timeline, speakers, video_context
        )
        
        # 4. –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–µ–º–∞–º –∏ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º
        structured_content = self.structure_content(corrected_timeline, speakers)
        
        # 5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        report = self.generate_chronological_report(structured_content, speakers)
        
        return {
            'speakers': speakers,
            'timeline': corrected_timeline,
            'structured_content': structured_content,
            'report': report
        }
    
    def identify_speakers(self, transcript_segments: List[Dict]) -> Dict[str, Speaker]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –≤—Å—Ç—Ä–µ—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 –º–∏–Ω—É—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        early_segments = []
        for segment in transcript_segments:
            if segment['start'] < 600:  # 10 –º–∏–Ω—É—Ç
                early_segments.append(segment)
        
        early_text = '\n'.join([s['text'] for s in early_segments])
        
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –≤—Å—Ç—Ä–µ—á–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤.

–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ø–µ—Ä–≤—ã—Ö –º–∏–Ω—É—Ç:
{early_text[:3000]}

–û–ø—Ä–µ–¥–µ–ª–∏:
1. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ (speakers)
2. –ò—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ (–µ—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è)
3. –ò—Ö —Ä–æ–ª–∏ (–≤–µ–¥—É—â–∏–π, –¥–æ–∫–ª–∞–¥—á–∏–∫, —É—á–∞—Å—Ç–Ω–∏–∫ –∏ —Ç.–¥.)
4. –•–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∏–ª–∏ —Å—Ç–∏–ª—å —Ä–µ—á–∏ –∫–∞–∂–¥–æ–≥–æ
5. –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–º–µ–Ω—ã –≥–æ–≤–æ—Ä—è—â–∏—Ö

–û—Ç–≤–µ—Ç—å –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{{
    "speakers": [
        {{
            "id": "speaker1",
            "probable_name": "–∏–º—è –µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ –∏–ª–∏ null",
            "role": "—Ä–æ–ª—å",
            "characteristics": ["—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ—á–∏"],
            "speech_patterns": ["–ø—Ä–∏–º–µ—Ä—ã —Ñ—Ä–∞–∑"]
        }}
    ],
    "speaker_change_indicators": ["–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–º–µ–Ω—ã –≥–æ–≤–æ—Ä—è—â–µ–≥–æ"],
    "meeting_format": "—Ñ–æ—Ä–º–∞—Ç –≤—Å—Ç—Ä–µ—á–∏ (–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è/–¥–∏—Å–∫—É—Å—Å–∏—è/–∏–Ω—Ç–µ—Ä–≤—å—é)"
}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã Speaker
            speakers = {}
            for speaker_data in result.get('speakers', []):
                speaker = Speaker(
                    id=speaker_data['id'],
                    name=speaker_data.get('probable_name'),
                    role=speaker_data.get('role', '—É—á–∞—Å—Ç–Ω–∏–∫'),
                    characteristics=speaker_data.get('characteristics', []),
                    voice_segments=[]
                )
                speakers[speaker.id] = speaker
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–º–µ–Ω—ã –≥–æ–≤–æ—Ä—è—â–∏—Ö
            self.speaker_change_indicators = result.get('speaker_change_indicators', [])
            self.meeting_format = result.get('meeting_format', 'discussion')
            
            # –¢–µ–ø–µ—Ä—å –ø—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º—É —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—É –∏ –Ω–∞–∑–Ω–∞—á–∞–µ–º –≥–æ–≤–æ—Ä—è—â–∏—Ö
            self.assign_speakers_to_segments(transcript_segments, speakers)
            
            return speakers
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–¥–Ω–æ–≥–æ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞
            default_speaker = Speaker(id="speaker1", name="–£—á–∞—Å—Ç–Ω–∏–∫", role="speaker")
            return {"speaker1": default_speaker}
    
    def assign_speakers_to_segments(self, transcript_segments: List[Dict], 
                                  speakers: Dict[str, Speaker]):
        """–ù–∞–∑–Ω–∞—á–∞–µ—Ç –≥–æ–≤–æ—Ä—è—â–∏—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞"""
        
        current_speaker = "speaker1"
        context_window = []
        
        for i, segment in enumerate(transcript_segments):
            # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = self.get_segment_context(transcript_segments, i, window_size=5)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞
            speaker_id = self.detect_speaker_for_segment(
                segment, context, speakers, current_speaker
            )
            
            segment['speaker_id'] = speaker_id
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö –≥–æ–≤–æ—Ä—è—â–µ–≥–æ
            if speaker_id in speakers:
                speakers[speaker_id].voice_segments.append(
                    (segment['start'], segment.get('end', segment['start'] + segment['duration']))
                )
            
            current_speaker = speaker_id
    
    def detect_speaker_for_segment(self, segment: Dict, context: List[Dict],
                                 speakers: Dict[str, Speaker], 
                                 current_speaker: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –Ω–∞—á–∞–ª–∞
        text_lower = segment['text'].lower()
        
        # –ò—â–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–º–µ–Ω—ã –≥–æ–≤–æ—Ä—è—â–µ–≥–æ
        for indicator in self.speaker_change_indicators:
            if indicator.lower() in text_lower:
                # –í–µ—Ä–æ—è—Ç–Ω–æ —Å–º–µ–Ω–∞ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ
                # –í—ã–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–æ —Ü–∏–∫–ª—É
                speaker_ids = list(speakers.keys())
                current_idx = speaker_ids.index(current_speaker)
                next_idx = (current_idx + 1) % len(speaker_ids)
                return speaker_ids[next_idx]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∫–∞–∂–¥–æ–≥–æ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ
        best_match = current_speaker
        best_score = 0
        
        for speaker_id, speaker in speakers.items():
            score = 0
            for characteristic in speaker.characteristics:
                if characteristic.lower() in text_lower:
                    score += 1
            
            if score > best_score:
                best_score = score
                best_match = speaker_id
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª–∏–Ω—É –ø–∞—É–∑
        if len(context) > 0:
            prev_segment = context[-1]
            pause = segment['start'] - (prev_segment['start'] + prev_segment.get('duration', 0))
            if pause > 2.0:  # –ü–∞—É–∑–∞ –±–æ–ª—å—à–µ 2 —Å–µ–∫—É–Ω–¥ - –≤–æ–∑–º–æ–∂–Ω–æ —Å–º–µ–Ω–∞ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ
                # –í–µ—Ä–æ—è—Ç–Ω–æ —Å–º–µ–Ω–∞
                speaker_ids = list(speakers.keys())
                if len(speaker_ids) > 1 and best_score == 0:
                    current_idx = speaker_ids.index(current_speaker)
                    next_idx = (current_idx + 1) % len(speaker_ids)
                    return speaker_ids[next_idx]
        
        return best_match if best_score > 0 else current_speaker
    
    def get_segment_context(self, segments: List[Dict], current_idx: int, 
                          window_size: int = 5) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        start_idx = max(0, current_idx - window_size)
        end_idx = min(len(segments), current_idx + window_size + 1)
        return segments[start_idx:end_idx]
    
    def create_timeline(self, transcript_segments: List[Dict], 
                       screenshots: List[Tuple]) -> List[TimelineEvent]:
        """–°–æ–∑–¥–∞–µ—Ç –µ–¥–∏–Ω—É—é –≤—Ä–µ–º–µ–Ω–Ω—É—é –ª–∏–Ω–∏—é –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤"""
        
        timeline = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
        for segment in transcript_segments:
            event = TimelineEvent(
                timestamp=segment['start'],
                type='transcript',
                content={
                    'text': segment['text'],
                    'duration': segment.get('duration', 0),
                    'speaker_id': segment.get('speaker_id', 'unknown'),
                    'original_segment': segment
                }
            )
            timeline.append(event)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
        for screenshot_path, timestamp, description, reason in screenshots:
            event = TimelineEvent(
                timestamp=float(timestamp),
                type='screenshot',
                content={
                    'path': screenshot_path,
                    'description': description,
                    'reason': reason
                },
                importance=0.8  # –°–∫—Ä–∏–Ω—à–æ—Ç—ã –æ–±—ã—á–Ω–æ –≤–∞–∂–Ω—ã
            )
            timeline.append(event)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        timeline.sort(key=lambda x: x.timestamp)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–µ–º
        self.detect_topic_changes(timeline)
        
        return timeline
    
    def detect_topic_changes(self, timeline: List[TimelineEvent]):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–æ–º–µ–Ω—Ç—ã —Å–º–µ–Ω—ã —Ç–µ–º—ã –æ–±—Å—É–∂–¥–µ–Ω–∏—è"""
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        text_groups = []
        current_group = []
        
        for event in timeline:
            if event.type == 'transcript':
                current_group.append(event)
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 10 —Å–µ–≥–º–µ–Ω—Ç–æ–≤
                if len(current_group) >= 10:
                    self.analyze_topic_group(current_group, timeline)
                    current_group = current_group[-5:]  # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥—Ä—É–ø–ø—É
        if current_group:
            self.analyze_topic_group(current_group, timeline)
    
    def analyze_topic_group(self, text_events: List[TimelineEvent], 
                          timeline: List[TimelineEvent]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä—É–ø–ø—É —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Ç–µ–º—ã"""
        
        texts = [e.content['text'] for e in text_events]
        combined_text = ' '.join(texts)
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º—ã
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        topic_keywords = {
            '–≤–≤–µ–¥–µ–Ω–∏–µ': ['–ø—Ä–∏–≤–µ—Ç', '–¥–æ–±—Ä—ã–π –¥–µ–Ω—å', '–Ω–∞—á–Ω–µ–º', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—é—Å—å'],
            '–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è': ['—Å–ª–∞–π–¥', '–ø–æ–∫–∞–∂—É', '–¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è', '–ø—Ä–∏–º–µ—Ä'],
            '–∫–æ–¥': ['–∫–æ–¥', '—Ñ—É–Ω–∫—Ü–∏—è', '–º–µ—Ç–æ–¥', '–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è', '–∫–ª–∞—Å—Å'],
            '–æ–±—Å—É–∂–¥–µ–Ω–∏–µ': ['–≤–æ–ø—Ä–æ—Å', '—á—Ç–æ –¥—É–º–∞–µ—Ç–µ', '–º–Ω–µ–Ω–∏–µ', '–æ–±—Å—É–¥–∏–º'],
            '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ': ['–∏—Ç–æ–≥', '–≤—ã–≤–æ–¥', '—Ä–µ–∑—é–º–µ', '—Å–ø–∞—Å–∏–±–æ', '–≤–æ–ø—Ä–æ—Å—ã']
        }
        
        detected_topic = None
        max_score = 0
        
        for topic, keywords in topic_keywords.items():
            score = sum(1 for kw in keywords if kw in combined_text.lower())
            if score > max_score:
                max_score = score
                detected_topic = topic
        
        if detected_topic and max_score > 2:
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ —Å–º–µ–Ω—ã —Ç–µ–º—ã
            topic_event = TimelineEvent(
                timestamp=text_events[0].timestamp,
                type='topic_change',
                content={'topic': detected_topic},
                importance=0.6
            )
            timeline.append(topic_event)
    
    def correct_transcript_with_context(self, timeline: List[TimelineEvent],
                                      speakers: Dict[str, Speaker],
                                      video_context: Dict) -> List[TimelineEvent]:
        """–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤"""
        
        logger.info("–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processing_groups = self.create_processing_groups(timeline)
        
        corrected_timeline = []
        
        for group in processing_groups:
            # –ù–∞—Ö–æ–¥–∏–º —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –≤ –≥—Ä—É–ø–ø–µ
            screenshots_in_group = [e for e in group if e.type == 'screenshot']
            transcript_in_group = [e for e in group if e.type == 'transcript']
            
            if not transcript_in_group:
                corrected_timeline.extend(group)
                continue
            
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã –≤ –≥—Ä—É–ø–ø–µ
            corrected_events = self.correct_transcript_group(
                transcript_in_group, screenshots_in_group, speakers, video_context
            )
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ —Å–æ–±—ã—Ç–∏—è–º–∏
            for event in group:
                if event.type == 'transcript':
                    # –ù–∞—Ö–æ–¥–∏–º –æ—Ç–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
                    for corrected in corrected_events:
                        if corrected.timestamp == event.timestamp:
                            corrected_timeline.append(corrected)
                            break
                else:
                    corrected_timeline.append(event)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        corrected_timeline.sort(key=lambda x: x.timestamp)
        
        return corrected_timeline
    
    def create_processing_groups(self, timeline: List[TimelineEvent], 
                               group_duration: float = 30.0) -> List[List[TimelineEvent]]:
        """–°–æ–∑–¥–∞–µ—Ç –≥—Ä—É–ø–ø—ã —Å–æ–±—ã—Ç–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        groups = []
        current_group = []
        group_start = 0
        
        for event in timeline:
            if not current_group:
                group_start = event.timestamp
                current_group.append(event)
            elif event.timestamp - group_start <= group_duration:
                current_group.append(event)
            else:
                groups.append(current_group)
                current_group = [event]
                group_start = event.timestamp
        
        if current_group:
            groups.append(current_group)
        
        return groups
    
    def correct_transcript_group(self, transcript_events: List[TimelineEvent],
                               screenshot_events: List[TimelineEvent],
                               speakers: Dict[str, Speaker],
                               video_context: Dict) -> List[TimelineEvent]:
        """–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –≥—Ä—É–ø–ø—É —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–µ—Ç –û–°–ú–´–°–õ–ï–ù–ù–´–ô —Ç–µ–∫—Å—Ç"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ –≥–æ–≤–æ—Ä—è—â–∏–º
        speakers_texts = {}
        timeline_context = []
        
        for event in transcript_events:
            speaker_id = event.content.get('speaker_id', 'unknown')
            speaker_name = speakers.get(speaker_id, Speaker(id=speaker_id)).name or f"–£—á–∞—Å—Ç–Ω–∏–∫ {speaker_id[-1] if speaker_id != 'unknown' else '1'}"
            
            if speaker_name not in speakers_texts:
                speakers_texts[speaker_name] = []
            
            speakers_texts[speaker_name].append(event.content['text'])
            timeline_context.append({
                'time': event.timestamp,
                'speaker': speaker_name,
                'text': event.content['text']
            })
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        visual_context = ""
        if screenshot_events:
            visual_context = "\n\n–í–ò–ó–£–ê–õ–¨–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ –Ω–∞ —ç–∫—Ä–∞–Ω–µ –≤ —ç—Ç–æ—Ç –ø–µ—Ä–∏–æ–¥:\n"
            for ss_event in screenshot_events:
                time_str = f"{int(ss_event.timestamp//60)}:{int(ss_event.timestamp%60):02d}"
                desc = ss_event.content.get('description', '')
                reason = ss_event.content.get('reason', '–∏–∑–º–µ–Ω–µ–Ω–∏–µ')
                visual_context += f"[{time_str}] {reason}: {desc}\n"
        
        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –û–°–ú–´–°–õ–ï–ù–ù–û–ô –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
        prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –≤–∏–¥–µ–æ –≤—Å—Ç—Ä–µ—á. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞—Ç—å –û–°–ú–´–°–õ–ï–ù–ù–´–ô, –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-—Å–≤—è–∑–∞–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç.

–¢–ò–ü –í–°–¢–†–ï–ß–ò: {video_context.get('meeting_type', '–æ–±—Å—É–∂–¥–µ–Ω–∏–µ')}
–û–°–ù–û–í–ù–´–ï –¢–ï–ú–´: {', '.join(video_context.get('main_topics', []))}

–£–ß–ê–°–¢–ù–ò–ö–ò:
{chr(10).join([f"- {name}: {len(texts)} —Ä–µ–ø–ª–∏–∫" for name, texts in speakers_texts.items()])}

–ò–°–•–û–î–ù–´–ô –î–ò–ê–õ–û–ì (–ø–æ –≤—Ä–µ–º–µ–Ω–∏):
{chr(10).join([f"[{int(item['time']//60)}:{int(item['time']%60):02d}] {item['speaker']}: {item['text']}" for item in timeline_context])}
{visual_context}

–ó–ê–î–ê–ß–ò –ö–û–†–†–ï–ö–¶–ò–ò:
1. **–ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏** - –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ —Ç–µ—Ä–º–∏–Ω—ã
2. **–î–æ–±–∞–≤—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É** - —Å–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç —á–∏—Ç–∞–µ–º—ã–º
3. **–£—á—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç** - –µ—Å–ª–∏ –≥–æ–≤–æ—Ä—è—Ç "—Å–º–æ—Ç—Ä–∏—Ç–µ —Å—é–¥–∞" –∏–ª–∏ "–∫–∞–∫ –≤–∏–¥–∏—Ç–µ", –¥–æ–±–∞–≤—å –ø–æ—è—Å–Ω–µ–Ω–∏–µ [–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —ç–∫—Ä–∞–Ω–µ...]
4. **–°–æ—Ö—Ä–∞–Ω–∏ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—é** - –ø–æ—Ä—è–¥–æ–∫ —Ä–µ–ø–ª–∏–∫ –¥–æ–ª–∂–µ–Ω –æ—Å—Ç–∞—Ç—å—Å—è —Ç–µ–º –∂–µ
5. **–î–æ–±–∞–≤—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–≤—è–∑–∫–∏** - –µ—Å–ª–∏ —Ä–µ–ø–ª–∏–∫–∏ —Å–≤—è–∑–∞–Ω—ã, –ø–æ–∫–∞–∂–∏ —ç—Ç–æ
6. **–£–±–µ—Ä–∏ –ø–æ–≤—Ç–æ—Ä—ã –∏ –ø–∞—É–∑—ã** - –æ—Å—Ç–∞–≤—å —Ç–æ–ª—å–∫–æ —Å–º—ã—Å–ª–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
7. **–î–æ–±–∞–≤—å –ø–æ—è—Å–Ω–µ–Ω–∏—è –≤ —Å–∫–æ–±–∫–∞—Ö** - –µ—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–ª–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã

–í–ê–ñ–ù–û: –°–æ–∑–¥–∞–π —Å–≤—è–∑–Ω—ã–π, –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥, –∞ –Ω–µ –Ω–∞–±–æ—Ä –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑!

–í–µ—Ä–Ω–∏ JSON —Å –æ—Ç–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏:
{{
    "corrected_segments": [
        {{
            "speaker": "–∏–º—è_–≥–æ–≤–æ—Ä—è—â–µ–≥–æ",
            "corrected_text": "–æ—Ç–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
            "context_explanation": "–ø–æ—è—Å–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ",
            "visual_reference": "—Å—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –µ—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è"
        }}
    ],
    "overall_context": "–æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–µ–≥–æ –≤ —ç—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º –æ—Ç—Ä–µ–∑–∫–µ"
}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.2,
                max_tokens=3000
            )
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            result = json.loads(response.choices[0].message.content)
            corrected_segments = result.get('corrected_segments', [])
            overall_context = result.get('overall_context', '')
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            corrected_events = []
            for i, event in enumerate(transcript_events):
                if i < len(corrected_segments):
                    corrected = corrected_segments[i]
                    
                    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π event
                    new_event = TimelineEvent(
                        timestamp=event.timestamp,
                        type='transcript',
                        content={
                            'text': event.content['text'],  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
                            'corrected_text': corrected.get('corrected_text', event.content['text']),
                            'speaker_id': event.content.get('speaker_id'),
                            'speaker_name': corrected.get('speaker'),
                            'context_explanation': corrected.get('context_explanation'),
                            'visual_reference': corrected.get('visual_reference'),
                            'duration': event.content.get('duration', 0),
                            'overall_context': overall_context if i == 0 else None  # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –∫ –ø–µ—Ä–≤–æ–º—É
                        },
                        importance=event.importance
                    )
                    corrected_events.append(new_event)
                else:
                    corrected_events.append(event)
            
            return corrected_events
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return transcript_events
    
    def structure_content(self, timeline: List[TimelineEvent], 
                         speakers: Dict[str, Speaker]) -> Dict:
        """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Ç–µ–º–∞–º –∏ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º"""
        
        structured = {
            'sections': [],
            'speaker_stats': defaultdict(lambda: {
                'word_count': 0,
                'segment_count': 0,
                'topics_discussed': set()
            }),
            'key_moments': []
        }
        
        current_section = {
            'start_time': 0,
            'end_time': 0,
            'topic': '–ù–∞—á–∞–ª–æ',
            'events': [],
            'speakers': set()
        }
        
        for event in timeline:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–º–µ–Ω—ã —Ç–µ–º—ã
            if event.type == 'topic_change':
                # –ó–∞–≤–µ—Ä—à–∞–µ–º —Ç–µ–∫—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_section['events']:
                    current_section['end_time'] = current_section['events'][-1].timestamp
                    structured['sections'].append(current_section)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
                current_section = {
                    'start_time': event.timestamp,
                    'end_time': event.timestamp,
                    'topic': event.content['topic'],
                    'events': [],
                    'speakers': set()
                }
            else:
                current_section['events'].append(event)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                if event.type == 'transcript':
                    speaker_id = event.content.get('speaker_id', 'unknown')
                    current_section['speakers'].add(speaker_id)
                    
                    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ
                    text = event.content.get('corrected_text', event.content.get('text', ''))
                    structured['speaker_stats'][speaker_id]['word_count'] += len(text.split())
                    structured['speaker_stats'][speaker_id]['segment_count'] += 1
                    structured['speaker_stats'][speaker_id]['topics_discussed'].add(
                        current_section['topic']
                    )
                
                # –û—Ç–º–µ—á–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
                if event.type == 'screenshot' or event.importance > 0.7:
                    structured['key_moments'].append({
                        'timestamp': event.timestamp,
                        'type': event.type,
                        'description': self.get_event_description(event)
                    })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        if current_section['events']:
            current_section['end_time'] = current_section['events'][-1].timestamp
            structured['sections'].append(current_section)
        
        return structured
    
    def get_event_description(self, event: TimelineEvent) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è"""
        
        if event.type == 'screenshot':
            return event.content.get('description', '–°–∫—Ä–∏–Ω—à–æ—Ç')
        elif event.type == 'transcript':
            text = event.content.get('corrected_text', event.content.get('text', ''))
            return text[:100] + '...' if len(text) > 100 else text
        elif event.type == 'topic_change':
            return f"–ü–µ—Ä–µ—Ö–æ–¥ –∫ —Ç–µ–º–µ: {event.content.get('topic', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
        else:
            return "–°–æ–±—ã—Ç–∏–µ"
    
    def generate_chronological_report(self, structured_content: Dict,
                                    speakers: Dict[str, Speaker]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç"""
        
        report = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        report.append("# –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –≤—Å—Ç—Ä–µ—á–∏\n")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É—á–∞—Å—Ç–Ω–∏–∫–∞—Ö
        report.append("## –£—á–∞—Å—Ç–Ω–∏–∫–∏\n")
        for speaker_id, speaker in speakers.items():
            stats = structured_content['speaker_stats'].get(speaker_id, {})
            name = speaker.name or f"–£—á–∞—Å—Ç–Ω–∏–∫ {speaker_id[-1]}"
            role = speaker.role or "—É—á–∞—Å—Ç–Ω–∏–∫"
            
            report.append(f"### {name} ({role})")
            report.append(f"- –°–∫–∞–∑–∞–Ω–æ —Å–ª–æ–≤: {stats.get('word_count', 0)}")
            report.append(f"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫: {stats.get('segment_count', 0)}")
            topics = list(stats.get('topics_discussed', set()))
            if topics:
                report.append(f"- –£—á–∞—Å—Ç–≤–æ–≤–∞–ª –≤ –æ–±—Å—É–∂–¥–µ–Ω–∏–∏: {', '.join(topics)}")
            report.append("")
        
        # –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
        if structured_content['key_moments']:
            report.append("\n## –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã\n")
            for moment in structured_content['key_moments'][:10]:  # –¢–æ–ø 10
                time = self.format_time(moment['timestamp'])
                report.append(f"- **{time}** - {moment['description']}")
            report.append("")
        
        # –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ø–æ —Å–µ–∫—Ü–∏—è–º
        report.append("\n## –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç\n")
        
        for section in structured_content['sections']:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏
            start_time = self.format_time(section['start_time'])
            end_time = self.format_time(section['end_time'])
            report.append(f"\n### {section['topic']} ({start_time} - {end_time})\n")
            
            # –°–æ–±—ã—Ç–∏—è –≤ —Å–µ–∫—Ü–∏–∏
            for event in section['events']:
                if event.type == 'transcript':
                    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
                    time = self.format_time(event.timestamp)
                    speaker_id = event.content.get('speaker_id', 'unknown')
                    speaker_name = event.content.get('speaker_name') or \
                                 speakers.get(speaker_id, Speaker(id=speaker_id)).name or \
                                 f"–£—á–∞—Å—Ç–Ω–∏–∫ {speaker_id[-1]}"
                    
                    text = event.content.get('corrected_text', event.content.get('text', ''))
                    context_note = event.content.get('context_explanation')
                    
                    report.append(f"**[{time}] {speaker_name}:** {text}")
                    if context_note:
                        report.append(f"*[{context_note}]*")
                    report.append("")
                    
                elif event.type == 'screenshot':
                    # –°–∫—Ä–∏–Ω—à–æ—Ç
                    time = self.format_time(event.timestamp)
                    description = event.content.get('description', '–°–∫—Ä–∏–Ω—à–æ—Ç')
                    reason = event.content.get('reason', '')
                    path = event.content.get('path', '')
                    
                    report.append(f"\nüì∏ **[{time}] –°–∫—Ä–∏–Ω—à–æ—Ç** - {reason}")
                    report.append(f"*{description}*")
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64 –¥–ª—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è
                    if path and os.path.exists(path):
                        try:
                            with open(path, "rb") as image_file:
                                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                                ext = os.path.splitext(path)[1].lstrip('.')
                                base64_url = f"data:image/{ext};base64,{encoded_string}"
                                report.append(f"![–°–∫—Ä–∏–Ω—à–æ—Ç]({base64_url})\n")
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                            report.append(f"![–°–∫—Ä–∏–Ω—à–æ—Ç]({path})\n")
                    else:
                        report.append("")
        
        return '\n'.join(report)
    
    def format_time(self, seconds: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes:02d}:{secs:02d}"

def integrate_chronological_processor(transcript_segments: List[Dict],
                                    screenshots: List[Tuple],
                                    video_context: Dict,
                                    api_key: str) -> Dict:
    """–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –≤ –æ—Å–Ω–æ–≤–Ω–æ–π pipeline"""
    
    processor = ChronologicalTranscriptProcessor(api_key)
    result = processor.process_video_meeting(
        transcript_segments, screenshots, video_context
    )
    
    return result

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_segments = [
        {"text": "–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç, –¥–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º –Ω–∞—à—É –≤—Å—Ç—Ä–µ—á—É", "start": 0, "duration": 3},
        {"text": "–ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω, —è –±—É–¥—É –≤–µ—Å—Ç–∏ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—é", "start": 3, "duration": 4},
        {"text": "–°–µ–≥–æ–¥–Ω—è –º—ã –æ–±—Å—É–¥–∏–º –Ω–æ–≤—ã–π –ø—Ä–æ–µ–∫—Ç", "start": 7, "duration": 3},
        {"text": "–£ –∫–æ–≥–æ –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã?", "start": 30, "duration": 2},
        {"text": "–î–∞, —É –º–µ–Ω—è –≤–æ–ø—Ä–æ—Å –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ", "start": 35, "duration": 3},
    ]
    
    test_screenshots = [
        ("screenshot_00010s.jpg", 10, "–°–ª–∞–π–¥ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏", "presentation_slide"),
        ("screenshot_00040s.jpg", 40, "–î–∏–∞–≥—Ä–∞–º–º–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–∏—Å—Ç–µ–º—ã", "diagram"),
    ]
    
    test_context = {
        'meeting_type': 'presentation',
        'main_topics': ['–Ω–æ–≤—ã–π –ø—Ä–æ–µ–∫—Ç', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞', '–ø–ª–∞–Ω—ã']
    }
    
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        processor = ChronologicalTranscriptProcessor(api_key)
        result = processor.process_video_meeting(
            test_segments, test_screenshots, test_context
        )
        
        print("=== –û–¢–ß–ï–¢ ===")
        print(result['report'])
    else:
        print("–¢—Ä–µ–±—É–µ—Ç—Å—è OPENAI_API_KEY")
